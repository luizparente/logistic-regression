%%%%%%%% ICML 2025 TEMPLATE USED FOR THIS PAPER %%%%%%%%%%%%%%%%%

\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line to anonymize the paper for peer review:
\usepackage{icml2025}

% For non-anonymized release, use this instead:
%\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Evaluating the Impact of Randomized Sampling on Training of Diverse Prediction Models}

\begin{document}

\twocolumn[
\icmltitle{Evaluating the Impact of Randomized Sampling on Training of Diverse Prediction Models}

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Luiz Parente}{equal,lp}
\end{icmlauthorlist}

\icmlaffiliation{lp}{School of Electrical and Computer Engineering, Purdue University, IN, USA}
\icmlcorrespondingauthor{Luiz Parente}{lparente@purdue.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, Sampling Algorithm, Leverage Scores}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}

In the modern world, data generation and collection have become ubiquitous, and machine learning algorithms are constantly employed to implement intelligent models for a diverse range of practical use-cases. Large corporations leverage customer data to predict consumption patterns. Health practitioners utilize medical datasets to diagnose sickness. Social media companies parse through \textit{likes} and \textit{comments} to analyze human behavior and maximize user engagement. The reliance on big-data has increasingly become the norm. However, as the available data grows in size and dimensionality, the computational power required to train such models increase proportionally. Therefore, optimizing compute efficiency when leveraging large-scale datasets to train machine learning prediction models becomes an obvious necessity as society shifts toward data-driven business models and paradigms. This study focuses on exploring \textbf{data sampling} as a possible solution for the problem of training prediction models on large data. The goal, given a dataset, is to derive a smaller subset that retains the main characteristics of the whole, allowing for the training of prediction models that provide comparable accuracy to those trained on the full dataset, however at a lower training cost.

\end{abstract}

\section{Introduction}

\textbf{Logistic regression} is a supervised learning technique commonly used to model the probability of an event occurring based on predictor variables. In the field of Machine Learning, logistic regression is often applied in the context of disease classification, anomaly detection, among others.

Despite being a well established and effective method for binary classification use-cases, handling large-scale datasets often present unique computational and algorithmic challenges, particularly in terms of time and space complexity, that demand specialized solutions. For example, logistic regression models typically employ optimization methods such as gradient descent to find the best-fitting parameters. The computational cost of each iteration is proportional to the number of observations and the number of features, often times making the optimization procedure computationally expensive.

With large datasets, the time complexity can become prohibitive, as the number of operations required for each iteration increases. Additionally, many logistic regression implementations require storing the data-points and intermediate computation results in memory. When working with big-data, this approach can lead to hardware bottlenecks. In particular, storing and manipulating large feature matrices can demand significant amounts of random-access memory (RAM), especially when the number of features is substantial. When the dataset exceeds memory limits, performance can degrade significantly, requiring the use of additional tooling, such as distributed computing or data sampling, as a means to overcome such challenges.

In this work, we explore the impacts of randomized sampling as an optimization strategy focused on addressing some of the challenges posed by high-dimensional datasets, specifically when the database size (e.g., number of records) is excessively large. The goal is to provide a comprehensive analysis rooted in extensive empirical observations made with a diverse collection of datasets and prediction methods, and evaluate whether the algorithms outlined by this study are effective in providing benefits when training machine learning prediction models.


\section{Related Work}

\subsection{Randomized Sampling}

The application of sampling algorithms to optimize training performance and manage dataset reduction has been a persistent area of research in machine learning, particularly in logistic regression and other high-dimensional classification problems. In recent studies, various methodologies have been proposed, including sophisticated leverage-based sampling techniques. For example, \citeauthor{chow24} introduce a randomized sampling algorithm specifically designed for logistic regression scenarios. Their approach leverages row-wise leverage scores to strategically select a subset of the data that maintains key dataset characteristics. The method provides theoretical guarantees for achieving high-quality approximations of the logistic regression model parameters, significantly reducing computational overhead while preserving predictive accuracy. This leverage-based approach contrasts with simpler methods, such as uniform sampling, by prioritizing influential data points that contribute most significantly to model accuracy.

\subsection{Signal Strength Leave-One-Out Estimator (SLOE)}

Building on methods aiming to enhance efficiency in high-dimensional logistic regression, \citeauthor{yad21} proposed the Signal Strength Leave-One-Out Estimator (SLOE). This method addresses challenges inherent in high-dimensional scenarios where traditional logistic regression estimators, like maximum likelihood estimation (MLE), typically perform poorly. The SLOE algorithm efficiently estimates a crucial signal strength parameter using a computationally efficient leave-one-out strategy, significantly outperforming prior heuristics in terms of both speed and accuracy. The authors validate their method through extensive simulations, highlighting its improved computational feasibility and its reliability in correcting biases and variance. The practical significance of SLOE is evident in high-dimensional logistic regression applications such as genomics and clinical diagnostics, where reliable statistical inference and rapid computation are both critical.

\subsection{Sample Reweighted Decorrelation Operator (SRDO)}

Addressing related stability concerns in training scenarios with mismatched data distributions, \citeauthor{shen20} introduced the Sample Reweighted Decorrelation Operator (SRDO). SRDO targets model instability caused by collinearity, a common issue in high-dimensional linear and logistic regression models. By intelligently reweighting samples, SRDO effectively reduces correlations among predictor variables, resulting in a design matrix that approximates orthogonality. This adjustment significantly mitigates instability and improves predictive accuracy when training and test datasets differ substantially in their underlying distributions. SRDO thus represents a complementary approach to dataset reduction techniques by ensuring robustness and stability in model predictions, particularly in environments prone to distribution shifts.

\subsection{Main Takeaways}

While these methods focus primarily on logistic regression and linear models, broader applicability across various predictive algorithms remains an open area for exploration. The leverage scores sampling method proposed by \citeauthor{chow24}, for instance, has not yet been thoroughly tested on non-linear models such as support vector machines, decision trees, or deep neural networks, where the composition of training data can profoundly impact model outcomes. Similarly, although SLOE has demonstrated significant efficiency gains in high-dimensional logistic regression, further research could explore its performance under non-Gaussian distributions and compare its effectiveness against traditional dimensionality reduction techniques, like principal component analysis or feature selection. The literature demonstrates a clear trend towards optimizing computational efficiency and robustness in high-dimensional regression problems through innovative sampling and reweighting strategies.


\section{Problem Definition}

The rapid growth in data volume has significantly impacted the computational demands placed on machine-learning-based systems, especially those requiring iterative parameter estimation methods, such as logistic regression. Large-scale datasets pose considerable challenges, including prolonged training times, increased resource consumption, and potential reductions in model interpretability and manageability. Consequently, there is a critical need to develop effective dataset reduction strategies that maintain the predictive accuracy of machine learning models while drastically improving computational efficiency.

This research explores solutions to the challenge of reducing the dataset size required for logistic regression and other classification models without incurring substantial losses in accuracy or stability. Traditional methods such as uniform random sampling, though computationally efficient, typically do not account for the inherent structure and importance of individual data points, potentially compromising model accuracy. Conversely, sophisticated sampling approaches leveraging data-driven metrics (e.g., leverage scores) offer theoretical guarantees for accuracy preservation but introduce complexity and additional computational overhead. Therefore, the primary research problem explored in this study is to systematically evaluate and quantify the trade-offs between computational complexity, training efficiency, and predictive performance when employing sampling algorithms with the specific goal of dataset reduction. This study investigates the efficacy of data-sampling strategies, determining their relative advantages and limitations across multiple classification algorithms, primarily logistic regression, but also support vector machines, K-nearest neighbors, and random forest classifiers.

Through comprehensive experimental analysis, this work seeks to answer a key question: To what extent can dataset reduction be achieved without significant accuracy degradation? Addressing this central question will enable a deeper understanding of dataset reduction techniques, providing valuable guidance for practitioners and researchers aiming to optimize the training processes of various classification models, particularly within the logistic regression framework.


\section{Methodology}

\subsection{Goals}

This study aims to explore potential solutions to the challenges posed by the use of high-dimensional datasets in the context of prediction model training. Objectively, we evaluate the impacts, implementations, and viable utilizations for randomized sampling algorithms in deriving smaller data subsets that retain the most representative characteristics of the original dataset, allowing for faster training of prediction models. If successful, this method can present a viable strategy for addressing many barriers faced when handling large databases in machine learning applications.


\subsection{Key Metrics}

To evaluate the impacts of the sampling algorithms in the training of prediction methods, a thorough benchmark is necessary to evaluate performance of models trained with sampled datasets relative to those trained on full datasets. To keep comparisons objective, we will focus on select metrics such as \textbf{accuracy} and \textbf{training time}. Formally:
$$
Accuracy(y, \hat{y}) = \frac{1}{n} \sum_{i=0}^{n-1} 1(\hat{y}_i = y_i)
$$

$$
Training\text{ }Time = s + t
$$

Where:

\begin{itemize}
	
	\item[$y$] is a vector representing the true values.
	\item[$\hat{y}$] is a vector representing the predictions.
	\item[$n$] is the number of predictions.
	\item[$s$] is the time consumed by the sampling algorithm. This is $zero$ for non-sampled models.
	\item[$t$] is the time consumed by the training process.
	
	
\end{itemize}

Moreover, each algorithm-dataset combination is run an adequate number of times for statistical relevance, and the key metrics are to be captured each time. Performance comparisons will be made with average values. This practice will allow for more consistent performance analysis across implementations. Specifically, the sampling algorithms will be tested with well-known open-source datasets, such as the Breast Cancer Wisconsin Diagnostic Dataset \cite{breastcancer}, the Iris Dataset \cite{iris} among others. A comprehensive and detailed review of each experiment conducted is included in \textbf{Appendix A}.

Lastly, we will assess whether the sampling algorithms contribute positively or negatively when used to train different prediction models beyond binary classification.


\subsection{Baseline Setup: Slow Logistic Regression}

In this study, we evaluate the impacts of a simplified version of the sampling algorithm proposed by \citet{chow24}, as well as an algorithm for true random sampling with uniform probability distribution, used for comparative analysis. These sampling algorithms are evaluated across a diverse variety of prediction models and databases, however, we here are primarily interested in assessing their applicability for predictions performed by \textbf{logistic regression} models. 

To set a performance baseline for our analysis, we start with our own implementation of the logistic regression algorithm, which is provided in Python code by class \texttt{SlowLogisticRegression}. It provides an abstraction that allows us to create a logistic regression prediction model.

By design, \texttt{SlowLogisticRegression} employs a simple implementation strategy, which is intentionally not optimized. The rationale behind this decision is that a slower model would allow for better visualization of the impacts of the sampling algorithms explored here, which are the focus of this work. In other words, we are interested in measuring the extent to which the sampling algorithms improve (or degrade) a logistic regression model that does not implement any other optimizations. This approach will allow us to isolate the impacts caused specifically by the sampling algorithms.


\subsubsection{Training Data}

The model expects an input of training data in the following format:
\begin{itemize}

\item \textbf{Features ($X$):} A matrix of input features where each row represents a data point and each column represents a feature (e.g., a matrix of shape $n \times m$, where $n$ is the number of data points and $m$ is the number of features).

\item \textbf{Labels ($y$):} A vector of length $n$ with the target binary labels (0 or 1), corresponding to the input features.
\end{itemize}


\subsubsection{Initialization}

Model parameters are initialized as follows:

\begin{itemize}
\item \textbf{Weights ($\theta$):} The weights (coefficients) of the model, here initialized as zeros. These weights are of size $m$ (one for each feature).

\item \textbf{Bias ($b$):} A scalar value added to the output of the linear combination of the features, here initialized to zero.

\end{itemize}


\subsubsection{Model Hypothesis}

A linear combination (linear model) is implemented for the inputs and weights.

\begin{itemize}

\item \textbf{Linear Combination:} For each data point, we compute the weighted sum of the features plus the bias term.
$$
z = \theta_1 x_1 + \theta_2 x_2 + \dots + \theta_m x_m + b
$$

\item \textbf{Sigmoid Function (Logistic Function):} This function maps the output of the linear combination to a probability between 0 and 1, which represents the probability of the positive class (prediction being $true$). We apply the sigmoid function to the linear combination $z$ to obtain the predicted probability:
$$
\hat{y}(z) = \frac{1}{1 + e^{-z}}
$$

\end{itemize}


\subsubsection{Loss Function}

The loss function used in our logistic regression implementation is the \textit{binary cross-entropy}, which measures the difference between the predicted probabilities versus the actual (expected) values. For a dataset of $n$ examples, the cost function $J(\theta)$ is given by:
$$
J(\theta) = -\frac{1}{n} \sum_{i=1}^{n} \left[ y^{(i)} \log(\hat{y}^{(i)}) + (1 - y^{(i)}) \log(1 - \hat{y}^{(i)}) \right]
$$

Where $y^{(i)}$ is the true label and $\hat{y}^{(i)}$ is the predicted probability for each data point.


\subsubsection{Optimization}

The goal of logistic regression is to find the optimal values for the weights and bias that minimize the loss function. In this work, this is done using \textit{gradient descent}, which goes as follows:

\begin{itemize}

\item[1.] Compute the gradients (partial derivatives) of the loss function with respect to each parameter (weights and bias):
$$
\frac{\partial J(\theta)}{\partial \theta_j} = \frac{1}{n} \sum_{i=1}^{n} \left( \hat{y}^{(i)} - y^{(i)} \right) x_j^{(i)}
$$
$$
\frac{\partial J(\theta)}{\partial b} = \frac{1}{n} \sum_{i=1}^{n} \left( \hat{y}^{(i)} - y^{(i)} \right)
$$

\item[2.] Update the weights and bias using the gradients, ($\alpha$ is the learning rate, which controls the step size of each update):
$$
\theta_j := \theta_j - \alpha \frac{\partial J(\theta)}{\partial \theta_j}
$$
$$
b := b - \alpha \frac{\partial J(\theta)}{\partial b}
$$

\item[3.] Repeat until the loss converges (i.e., the change in the cost function between iterations is small enough) or a predefined number of iterations is reached.

\end{itemize}


\subsubsection{Model Evaluation}

We evaluate the model’s performance by comparing the predicted labels against the actual values. Common metrics include accuracy, precision, recall, F1 score, among others.


\subsubsection{Output}

After training, the learned weights and bias can be used to make predictions on new data. The output is a probability, but for classification, a \textbf{decision boundary} (commonly $0.5$) is applied to convert the probability to a binary class label:
$$
\hat{y} = \begin{cases}
1 & \text{if } \hat{y} \geq 0.5 \\
0 & \text{if } \hat{y} < 0.5
\end{cases}
$$


\subsection{Leverage Scores Random Sampler}

\subsubsection{Overview}

At a high level, the goal of the sampling algorithm proposed by \citeauthor{chow24} is to derive a sampled subset that efficiently approximate the full dataset. This is achieved by performing Singular Value Decomposition (SVD) on the data matrix $X$ to calculate leverage scores, then using it to sample a subset of the data. Finally, the sampled data is used to train a logistic regression model, which is expected to closely replicate the performance of a model trained on the entire dataset.

To better analyze the algorithm and assess its transportability to different prediction models, we encapsulate our simplified implementation, which is provided in Python code, in class \texttt{BasicLeverageScoresSampler}. This class represents the first iteration of the randomized sampling algorithm on which this study is focused. However, this version \textbf{simplifies} the paper's approach. Such simplification will be useful later when benchmarking different implementations and analyzing trade-offs.


\subsubsection{Sampling Method}

To estimate the relevance of each data-point in the dataset, the Singular Value Decomposition (SVD) method is applied. The matrix $X$ is decomposed into its singular vectors and values, and the leverage score for each dataset entry (row in the matrix) is calculated as the sum of the squares of the elements in the corresponding row of the left singular vector matrix. This step directly follows the approach proposed by \citet{chow24}. Specifically, the paper states that leverage scores can be computed using SVD to capture the importance of each data point in the dataset with respect to the model.

\textbf{Algorithm 1} is a simplified version of the sampling algorithm proposed by \citet{chow24}. The computed leverage scores are first normalized to create a probability distribution that sums to $1$. This normalized distribution is then used to sample rows from the dataset $X$. The number of rows sampled is determined by a customizeable parameter. Rows with higher leverage scores are more likely to be chosen for the sample, ensuring that important data points (those that affect the model the most) are more likely to be included in the subsampled dataset.

\begin{algorithm}[tb]
	\caption{Basic Leverage Scores Sampling}
	\label{alg:example}
	\begin{algorithmic}
		\STATE {\bfseries Input:} feature-matrix $X$, target-labels $y$, sample rate $r$
		
		\vspace{0.25cm}
		
		\STATE $scores \leftarrow$ \texttt{leverage\_scores($X$)}
		\STATE $normalized\_p \leftarrow$ $scores$ / \texttt{sum($scores$)}
		\STATE $sampled\_indices \leftarrow$ \texttt{random\_select($X$, $r$,  $normalized\_p$)}
		\STATE $X\_sampled \leftarrow X[sampled\_indices]$
		\STATE $y\_sampled \leftarrow y[sampled\_indices]$
		
		\vspace{0.25cm}
		
		\STATE {\bfseries Return:} \texttt{$X\_sampled, y\_sampled$}
	\end{algorithmic}
\end{algorithm}


The implementation provided by \texttt{BasicLeverageScoresSampler} simplifies the original strategy taken by \citet{chow24}. While it is arguably \textbf{more computationally efficient}, it does not fully capture the optimizations and error guarantees of the paper's more complex sampling model. The approach proposed by the authors would be more appropriate for high-dimensional datasets or applications where precise error bounds and model accuracy are critical. However, for many practical purposes, the simpler implementation can provide a reasonable trade-off between speed and approximation accuracy.

More specifically, the paper describes a more detailed approach where not only the leverage scores are used to sample the data, but also a \textit{sketching matrix}, which allows the logistic regression model to be approximated more efficiently by reducing the dimensionality of the problem. The full matrix is used to modify how the data is projected into a lower-dimensional space, enabling faster training. In contrast, our initial implementation skips the step of constructing the sketching matrix. This simplifies the implementation, but results in a slightly less optimized solution as a trade-off.

In contemplation of the approach proposed by our initial implementation, we identify key advantages to simplifying the original algorithm:

\begin{itemize}

\item \textbf{Simplicity:} The implementation here is simpler and easier to understand. It directly samples rows based on leverage scores, which is computationally efficient and suitable for smaller datasets or less complex problems.

\item \textbf{Performance:} By using only leverage score sampling without constructing a sketching matrix, the algorithm arguably runs faster, since it requires fewer computations. This makes it suitable for applications where performance is more critical than exact accuracy.

\end{itemize}

We also acknowledge the trade-offs between simplicity and accuracy:

\begin{itemize}

\item \textbf{Accuracy:} Because the simplified implementation skips the step of constructing the sketching matrix, the approximation may not be as accurate as the one described in the paper. Without the full matrix, the approximation quality could be compromised, especially for large, high-dimensional datasets.

\item \textbf{Theoretical guarantees:} Unlike the paper, this implementation does not provide formal guarantees on the error bounds, which are important for accuracy-critical applications.

\end{itemize}


\subsection{Uniform Random Sampler}

\subsubsection{Overview}

To effectively benchmark advanced data reduction algorithms, it is essential to understand whether there are advantages offered by the added selection complexity of such algorithms relative to a baseline method that employs a purely random selection strategy. For this purpose, a random sampling algorithm with uniform selection probability distribution is constructed. It provides this baseline by uniformly sampling dataset rows without considering any data-specific metrics or characteristics. This sampling technique is designed explicitly to evaluate the inherent performance implications of dataset size reduction independently of data-driven heuristics, thus offering a neutral point of comparison against more complex sampling methods, such as those utilizing leverage scores or other importance metrics.

Our implementation of the Uniform Random Sampler is encapsulated within a concise Python class, \texttt{RandomSampler}, intended to replicate simple, unbiased selection behavior. This simplicity allows us to quantify the improvements gained through more sophisticated sampling strategies relative to a truly random baseline. Consequently, performance variations observed between models trained on uniformly sampled data and those using leverage-based or feature-driven sampling methods directly reflect the added benefit of informed data-point selection.

\subsubsection{Sampling Method}

The fundamental principle underlying the Uniform Random Sampler algorithm is the equal likelihood of selection for each row of the dataset, ensuring unbiased representational distribution. Given a feature matrix $X$ of dimensions $(n_{samples} \times n_{features})$ and an associated target vector $y$, the Uniform Random Sampler selects a subset containing a predefined fraction of the original dataset, denoted as the sample percentage. Crucially, this sampling procedure employs uniform probabilities, assigning identical selection likelihood to every data entry, irrespective of feature values, variance, or distribution.

Formally, the selection process calculates the number of samples to extract, determined by the sample percentage multiplied by the total number of rows in the dataset. Subsequently, a set of unique indices is drawn randomly without replacement from the range of available indices. The resulting sampled subset, represented by the matrices $X_{sampled}$ and $y_{sampled}$, is then utilized to train the corresponding machine learning model. The simplicity of this method ensures its computational efficiency, making it highly suitable for large-scale datasets as a baseline sampling procedure.

The implementation of the uniform random sampling algorithm is detailed in \textbf{Algorithm 2}. Initially, it calculates the exact number of samples required based on the user-specified sample rate. Next, a uniform random selection without replacement is performed on the available indices, yielding an unbiased subset of dataset rows. Finally, the sampled indices are employed to extract the corresponding rows from the feature and target matrices.

\begin{algorithm}[tb]
	\caption{Uniform Random Sampling}
	\label{alg:random_alg}
	\begin{algorithmic}
		\STATE {\bfseries Input:} feature-matrix $X$, target-labels $y$, sample rate $r$
		
		\vspace{0.25cm}
		
		\STATE $sampled\_indices \leftarrow$ \texttt{random\_select($X$, $r$)}
		\STATE $X\_sampled \leftarrow X[sampled\_indices]$
		\STATE $y\_sampled \leftarrow y[sampled\_indices]$
		
		\vspace{0.25cm}
		
		\STATE {\bfseries Return:} \texttt{$X\_sampled, y\_sampled$}
	\end{algorithmic}
\end{algorithm}


The explicit introduction of uniform random sampling as a baseline significantly enriches comparative studies focused on dataset reduction. It enables precise quantification of the value added by complex selection strategies, such as those leveraging SVD-derived scores, feature importance, or prediction uncertainty. Consequently, due to the experimental nature of this study, we can more conclusively demonstrate the impacts attributable to the various sampling methods by explicitly contrasting their results against a standardized, unbiased baseline.


\section{Experimental Results}

\subsection{Experiments}

To assess the efficacy of the sampling algorithms explored by this study, comprehensive experiments have been conducted as detailed in Appendix A. This section provides a short summary of observations for each set of experiments.

\begin{table}[t]
	\caption{Experiments conducted for logistic regression using \texttt{SlowLogisticRegression} and corresponding average accuracy $Acc$ and training time $t_{training}$ for leverage scores random sampling and uniform random sampling.}
	\label{t1}
	\vskip 0.15in
	\begin{center}
		\begin{small}
			\begin{sc}
				\begin{tabular}{lcccr}
					\toprule
					Exp. \# & Sampling & $Accuracy$ & $Training \text{ } Time$\\
					\midrule
					1 & None & 0.97 $\pm$ 0.00 & 0.26 $\pm$ 0.04 s \\
					2 & Lev. Scores & 0.96 $\pm$ 0.04 & 0.12 $\pm$ 0.01 s \\
					5 & Uniform & 0.96 $\pm$ 0.04 & 0.13 $\pm$ 0.01 s \\
					\bottomrule
				\end{tabular}
			\end{sc}
		\end{small}
	\end{center}
	\vskip -0.1in
\end{table}

\textbf{Table 1} shows the series of experiments conducted with \texttt{SlowLogisticRegression} over the Breast Cancer Wisconsin Diagnostic Dataset \cite{breastcancer}. We observe significant reduction in training time, at the negligible cost of 1 percent in prediction accuracy for both \texttt{BasicLeverageScoresSampler} and \texttt{RandomSampler}. This experiment does not identify statistically relevant differences between the two sampling algorithms.

\begin{table}[t]
	\caption{Experiments conducted for logistic regression using SciKit-Learn's \texttt{LogisticRegression} and corresponding average accuracy $Acc$ and training time $t_{training}$ for leverage scores random sampling and uniform random sampling.}
	\label{t2}
	\vskip 0.15in
	\begin{center}
		\begin{small}
			\begin{sc}
				\begin{tabular}{lcccr}
					\toprule
					Exp. \# & Sampling & $Accuracy$ & $Training \text{ } Time$\\
					\midrule
					3 & None & 0.97 $\pm$ 0.00 & 1.88 $\pm$ 0.83 ms \\
					4 & Lev. Scores & 0.94 $\pm$ 0.05 & 0.82 $\pm$ 0.38 ms \\
					\bottomrule
				\end{tabular}
			\end{sc}
		\end{small}
	\end{center}
	\vskip -0.1in
\end{table}

\textbf{Table 2} shows the series of experiments conducted with SciKit-Learn's \texttt{LogisticRegression} model over the Breast Cancer Wisconsin Diagnostic Dataset \cite{breastcancer}. We observe significant reduction in training time, at the cost of 3 percent in prediction accuracy.

Next, we explore the impact of the randomized sampling algorithms in three typical classification algorithms, namely K-Nearest Neighbors, Support Vector Machine, and Random Forest Classification.

\begin{table}[t]
	\caption{Experiments conducted for K-Nearest Neighbors using SciKit-Learn's \texttt{KNeighborsClassifier} and corresponding average accuracy $Acc$ and training time $t_{training}$ for leverage scores random sampling and uniform random sampling.}
	\label{t3}
	\vskip 0.15in
	\begin{center}
		\begin{small}
			\begin{sc}
				\begin{tabular}{lcccr}
					\toprule
					Exp. \# & Sampling & $Accuracy$ & $Training \text{ } Time$\\
					\midrule
					7 & None & 1.00 $\pm$ 0.00 & 1.03 $\pm$ 0.64 ms \\
					8 & Lev. Scores & 0.94 $\pm$ 0.11 & 0.89 $\pm$ 0.57 ms \\
					9 & Uniform & 0.93 $\pm$ 0.11 & 0.81 $\pm$ 0.40 ms \\
					\bottomrule
				\end{tabular}
			\end{sc}
		\end{small}
	\end{center}
	\vskip -0.1in
\end{table}

\textbf{Table 3} shows the series of experiments conducted with SciKit-Learn's \texttt{KNeighborsClassifier} model over the Iris Dataset \cite{iris}. We observe moderate reduction in training time, at the cost of 6 percent in prediction accuracy for \texttt{BasicLeverageScoresSampler}, and 7 percent for \texttt{RandomSampler}. This experiment indicates uniform random sampling provides faster training time compared to leverage scores sampling.

\begin{table}[t]
	\caption{Experiments conducted for Support Vector Machine using SciKit-Learn's \texttt{SVC} and corresponding average accuracy $Acc$ and training time $t_{training}$ for leverage scores random sampling and uniform random sampling.}
	\label{t4}
	\vskip 0.15in
	\begin{center}
		\begin{small}
			\begin{sc}
				\begin{tabular}{lcccr}
					\toprule
					Exp. \# & Sampling & $Accuracy$ & $Training \text{ } Time$\\
					\midrule
					10 & None & 1.00 $\pm$ 0.00 & 1.48 $\pm$ 0.69 ms \\
					11 & Lev. Scores & 0.86 $\pm$ 0.16 & 1.27 $\pm$ 0.69 ms \\
					12 & Uniform & 0.86 $\pm$ 0.16 & 1.56 $\pm$ 1.02 ms \\
					\bottomrule
				\end{tabular}
			\end{sc}
		\end{small}
	\end{center}
	\vskip -0.1in
\end{table}

\textbf{Table 4} shows the series of experiments conducted with SciKit-Learn's \texttt{SVC} model over the Iris Dataset \cite{iris}. We observe no reduction in training time, at the significant cost of 14 percent in prediction accuracy for both sampling algorithms.

\begin{table}[t]
	\caption{Experiments conducted for Random Forest Classifier using SciKit-Learn's \texttt{RandomForestClassifier} and corresponding average accuracy $Acc$ and training time $t_{training}$ for leverage scores random sampling and uniform random sampling.}
	\label{t5}
	\vskip 0.15in
	\begin{center}
		\begin{small}
			\begin{sc}
				\begin{tabular}{lcccr}
					\toprule
					Exp. \# & Sampling & $Accuracy$ & $Training \text{ } Time$\\
					\midrule
					13 & None & 1.00 $\pm$ 0.00 & 0.15 $\pm$ 0.02 ms \\
					14 & Lev. Scores & 0.93 $\pm$ 0.10 & 0.13 $\pm$ 0.01 ms \\
					15 & Uniform & 0.94 $\pm$ 0.10 & 0.12 $\pm$ 0.01 ms \\
					\bottomrule
				\end{tabular}
			\end{sc}
		\end{small}
	\end{center}
	\vskip -0.1in
\end{table}

\textbf{Table 5} shows the series of experiments conducted with SciKit-Learn's \texttt{RandomForestClassifier} model over the Wine Dataset \cite{wine}. We observe marginal reduction in training time, at the substantial cost of 7 percent in prediction accuracy for \texttt{BasicLeverageScoresSampler} and 6 percent for \texttt{RandomSampler}. No statistically significant differences have been identified between the two sampling algorithms in this experiment.


\subsection{Results Analysis}

The initial set of experiments involved using the \texttt{SlowLogisticRegression} model applied to the Breast Cancer Wisconsin Diagnostic Dataset \cite{breastcancer}. Results indicated that both leverage scores sampling and uniform random sampling achieved comparable outcomes, reducing training time approximately by half (from $0.26 \pm 0.04$ s to roughly $0.12 \pm 0.01$ s). This substantial reduction in computational effort was accompanied by a minimal accuracy loss of merely one percent. The negligible differences in accuracy observed between leverage-based and uniformly random sampling methods suggest that, for logistic regression scenarios similar to the one studied here, the advantage of informed leverage scores sampling might be limited, especially considering its computational overhead.

Further experiments assessed the impact of leverage scores sampling using the SciKit-Learn implementation of \texttt{LogisticRegression}, again employing the Breast Cancer Wisconsin Diagnostic Dataset \cite{breastcancer}. Here, a considerable reduction in training time was observed—from $1.88 \pm 0.83$ ms to $0.82 \pm 0.38$ ms when leveraging scores-based sampling was applied—amounting to approximately a 56\% reduction. Nonetheless, this computational improvement was accompanied by a notable accuracy decline of around three percent, from $0.97$ to $0.94$. Although this trade-off might be acceptable in computationally intensive practical scenarios, it underscores a potential limitation of these sampling approaches when predictive accuracy remains the primary concern, particularly in critical decision-making contexts.

Additional experiments involved applying sampling methods to the K-Nearest Neighbors classifier using SciKit-Learn's \texttt{KNeighborsClassifier} model and the Iris dataset \cite{iris}. These outcomes reinforced the trade-off between computational efficiency and predictive accuracy. Training time was moderately reduced with leverage-based sampling ($0.89 \pm 0.57$ ms) and further reduced with uniform sampling ($0.81 \pm 0.40$ ms), compared to the original $1.03 \pm 0.64$ ms. However, these reductions came with notable accuracy penalties—six percent for leverage-based sampling and seven percent for uniform sampling. Interestingly, uniform sampling marginally outperformed leverage sampling in terms of speed, indicating that simpler random selection algorithms might occasionally be more computationally efficient for lightweight classification models, though accuracy degradation remains a significant factor.

Further analysis addressed the use of sampling methods with the Support Vector Machine with SciKit-Learn's (\texttt{SVC}) model on the Iris dataset \cite{iris}. Here, both sampling algorithms failed to deliver meaningful computational savings, with training time remaining roughly unchanged compared to using the entire dataset. More critically, a substantial drop in accuracy (14 percent) was observed for both leverage scores and uniform random sampling, declining from a perfect accuracy of 1.00 to an average of 0.86. This outcome clearly illustrates a limitation when applying dataset-reduction sampling methods to kernel-based models such as SVM, whose accuracy is highly sensitive to the specific training instances included, thus highlighting the need for caution in similar contexts.

Lastly, experiments focused on applying sampling methods to a Random Forest Classifier trained with SciKit-Learn's \texttt{RandomForestClassifier} model and the Wine dataset \cite{wine}. Both sampling methods yielded minor training time improvements, with uniform sampling slightly outperforming leverage-based sampling ($0.12 \pm 0.01$ ms versus $0.13 \pm 0.01$ ms), compared to the original training time of $0.15 \pm 0.02$ ms. However, the accuracy reductions of seven percent for leverage-based sampling and six percent for uniform random sampling represented significant declines. No statistically significant difference between the two sampling methods was identified, suggesting that uniform random sampling might be equally effective—or ineffective—in this context, without the added computational complexity of leverage-based sampling.

Overall, these experiments suggest that random sampling algorithms, whether informed by leverage scores or employing uniform selection probability, effectively reduce computational training effort, though often at a variable cost to model accuracy, strongly dependent upon the specific learning algorithm and dataset characteristics. Logistic regression models demonstrated the most promising results, exhibiting minimal accuracy degradation, whereas methods sensitive to the training set's composition, such as SVM and Random Forest, exhibited more substantial accuracy losses. Consequently, while these sampling approaches are valuable tools for dataset reduction in computationally intensive scenarios, careful evaluation and analysis of trade-offs specific to each application domain remain essential for their effective and practical deployment.

\section{Contribution}

The main focus of this work is to provide an implementation of the leverage scores sampling algorithm proposed by \citet{chow24} for \textbf{logistic regression}. Nevertheless, our contribution extends to additionally include the following items:

\begin{itemize}
	\item \textbf{A comparative analysis} of how the leverage-scores-based algorithm performs versus a random selection algorithm with uniform probability distribution, and whether advantages can be perceived in employing the more complex selection strategy of leverage scores when sampling the data.
	
	\item \textbf{An empirical analysis} of whether these sampling algorithms provide the same advantages observed for logistic regression when training other classification models, namely K-Nearest Neighbors, Support Vector Machine, and Random Forest Classification.
\end{itemize}

These efforts aim to extrapolate the study of applicability of random sampling algorithms in the context of classification models, and provide a hands-on exploration of the various use-cases through the experiments designed for this study.


\section{Conclusion and Future Direction}

This research work systematically explored the application of random sampling algorithms as a strategic attempt to effectively address the persistent and challenging problem associated with training machine learning models using datasets of exceptionally high magnitude. Through the rigorous experiments and analyses conducted here, we have gained valuable insights into the potential advantages and tangible benefits of employing random sampling methods, particularly highlighted by the findings observed in the context of logistic regression models. Specifically, these random selection algorithms have consistently demonstrated their capability to serve as a feasible and practical solution, effectively reducing the volume of the training dataset without causing critically significant loss in predictive quality or accuracy. 

It is essential and particularly noteworthy to mention, however, that careful consideration is due when determining whether the inevitable loss in predictive accuracy introduced by dataset reduction remains acceptable, given the specific circumstances and demands of the use-case at hand. For example, scenarios involving academic or theoretical research might prioritize predictive accuracy as an indispensable component for a study, whereas practical, industry-focused applications might alternatively prioritize computational speed and reduced training time as key performance metrics, thus tolerating a slightly lower accuracy for the sake of faster model training and deployment. Nevertheless, the results presented herein clearly demonstrate the viability and practicality of random sampling methods as effective techniques in machine learning dataset reduction. 

Future research efforts can build upon these findings by further strengthening technical guarantees that increase representativeness of the reduced dataset by optimizing leverage scores selection while considering numerical relevance of the original records. Moreover, there could be another pathway in evaluating other strategies in dimensionality reduction that go beyond random sampling methods, and examining the various trade-offs and validating these algorithms across diverse real-world scenarios to consolidate their practical applicability can provide substantial contribution to the field of Machine Learning.


\section*{Software and Data}

The source code for this work can be downloaded at:

\url{https://github.com/luizparente/logistic-regression}


\bibliography{paper_references}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\twocolumn
\section{Experiments}

\subsubsection{Objective Metrics}

The goal of the experiments described here is to record key metrics for each model-dataset combination, namely \textbf{accuracy} and \textbf{training time}. Formally, we define:
$$
Accuracy(y, \hat{y}) = \frac{1}{n} \sum_{i=0}^{n-1} 1(\hat{y}_i = y_i)
$$

$$
Training\text{ }Time = s + t
$$

Where:

\begin{itemize}

\item[$y$] is a vector representing the true values.
\item[$\hat{y}$] is a vector representing the predictions.
\item[$n$] is the number of predictions.
\item[$s$] is the time consumed by the sampling algorithm. This is $zero$ for non-sampled models.
\item[$t$] is the time consumed by the training process.

\end{itemize}

Additionally, each experiment defines its own set of parameters and leverage different algorithm implementations.

\subsection{Experiment 1: \texttt{SlowLogisticRegression}'s Performance, No Sampling}


\subsubsection{Objective}

The experiment aims to assess the performance of a custom implementation of logistic regression, named \texttt{SlowLogisticRegression}, in terms of classification accuracy and training time across multiple runs on an open-source dataset \cite{breastcancer}. The experiment seeks to quantify the model's generalization ability and computational efficiency under standardized conditions.


\subsubsection{Materials and Methods}

The experiment utilizes a dataset represented by feature matrix $ X \in \mathbb{R}^{n \times m} $ and target vector $ y \in \{0, 1\}^n $, where $ n $ is the number of samples and $ m $ is the number of features. The dataset is assumed to be preprocessed and suitable for binary classification tasks.

For each experimental run, the dataset is split into training and test sets using an 80-20 partition, with 80\% of the data allocated to training ($ X_{\text{train}}, y_{\text{train}} $) and 20\% to testing ($ X_{\text{test}}, y_{\text{test}} $). The split is performed using a fixed random seed of 42 to ensure reproducibility across runs. Feature scaling is applied using Python's SciKit-Learn \texttt{MinMaxScaler}, which normalizes each feature to the range $[0, 1]$ based on the training set's minimum and maximum values. The scaling transformation is computed as:
$$
X'_{i,j} = \frac{X_{i,j} - \min(X_{:,j})}{\max(X_{:,j}) - \min(X_{:,j})}
$$

where $ X_{i,j} $ denotes the $ j $-th feature of the $ i $-th sample, and the transformation is fitted on $ X_{\text{train}} $ and applied to both $ X_{\text{train}} $ and $ X_{\text{test}} $.


\subsubsection{Model and Training}

The classification model employed is a custom logistic regression implementation (\texttt{SlowLogisticRegression}), parameterized by a learning rate of 0.1 and a fixed number of 5000 training epochs. The implementation of this model is detailed in section 2 of this paper.


\subsubsection{Experimental Procedure}

The experiment is repeated 100 times for statistical relevance. For each iteration:

\begin{itemize}
 \item [1.] The dataset is split into training and test sets.
 \item [2.] Features are scaled using \texttt{MinMaxScaler}.
 \item [3.] The \texttt{SlowLogisticRegression} model is initialized and trained on the scaled training data, with training time recorded.
 \item [4.] Predictions are generated on the scaled test set.
 \item [5.] Accuracy is calculated by comparing predictions to the true test labels.
 \item [6.] Accuracy and training time are tracked separately.
\end{itemize}

This procedure records the accuracy scores and training times across each cycle, which are later used to calculate the average values for each metric.


\subsubsection{Analysis}

The collected accuracies and training times enable statistical analysis of the model's performance. Key statistics, such as the mean and standard deviation of accuracy, can be computed to evaluate consistency and generalization. Similarly, training time statistics provide insight into the computational cost of the \texttt{SlowLogisticRegression} implementation without any sampling.


\subsubsection{Assumptions and Limitations}

The experiment assumes that the dataset is suitable for binary classification and that the features benefit from min-max scaling. The fixed hyperparameters (learning rate = 0.1, epochs = 5000), obtained empirically, appear to suit this experiment well and provide satisfactory results.


\subsubsection{Expected Outcomes}

The experiment is expected to yield a distribution of accuracy scores reflecting the model's predictive capability and a distribution of training times indicating its efficiency. These results can be used to benchmark \texttt{SlowLogisticRegression} against other implementations or algorithms under identical conditions.

\subsubsection{Measured Outcomes}

The expected outcomes for this experiment have been empirically confirmed, as illustrated in \textbf{Figure 1}.

\begin{figure}[ht]
	\vskip 0.2in
	\begin{center}
		\centerline{\includegraphics[width=\columnwidth]{experiment_01}}
		\caption{Mean accuracies and training times observed over 100 training cycles of \texttt{SlowLogisticRegression}, without any sampling, using the Breast Cancer Wisconsin Diagnostic Dataset \cite{breastcancer}.}
		\label{experiment_01}
	\end{center}
	\vskip -0.2in
\end{figure}


\subsection{Experiment 2: \texttt{SlowLogisticRegression}'s Performance with Leverage Score Sampling}

\subsubsection{Objective}

The experiment aims to assess the performance of a custom implementation of logistic regression, named \texttt{SlowLogisticRegression}, in terms of classification accuracy and training time across multiple runs on an open-source dataset \cite{breastcancer}. The experiment seeks to quantify the model's generalization ability and computational efficiency under standardized conditions, incorporating a data sampling step using the randomized sampling algorithm \texttt{BasicLeverageScoresSampler}.


\subsubsection{Materials and Methods}

The experiment utilizes a dataset represented by feature matrix $ X \in \mathbb{R}^{n \times m} $ and target vector $ y \in \{0, 1\}^n $, where $ n $ is the number of samples and $ m $ is the number of features. The dataset is assumed to be preprocessed and suitable for binary classification tasks.

For each experimental run, the dataset is first sampled to 20\% of its original size using the \texttt{BasicLeverageScoresSampler}, resulting in a reduced dataset ($ X_{\text{sampled}}, y_{\text{sampled}} $). This sampled dataset is then split into training and test sets using an 80-20 partition, with 80\% of the sampled data allocated to training ($ X_{\text{train}}, y_{\text{train}} $) and 20\% to testing ($ X_{\text{test}}, y_{\text{test}} $). The split is performed using a fixed random seed of 42 to ensure reproducibility across runs. Feature scaling is applied using Python's SciKit-Learn \texttt{MinMaxScaler}, which normalizes each feature to the range $[0, 1]$ based on the training set's minimum and maximum values. The scaling transformation is computed as:
\[
X'_{i,j} = \frac{X_{i,j} - \min(X_{:,j})}{\max(X_{:,j}) - \min(X_{:,j})}
\]
where $ X_{i,j} $ denotes the $ j $-th feature of the $ i $-th sample, and the transformation is fitted on $ X_{\text{train}} $ and applied to both $ X_{\text{train}} $ and $ X_{\text{test}} $.

\subsubsection{Model and Training}

The classification model employed is a custom logistic regression implementation (\texttt{SlowLogisticRegression}), parameterized by a learning rate of 0.1 and a fixed number of 5000 training epochs. The implementation of this model is detailed in section 2 of this paper.

\subsubsection{Experimental Procedure}

The experiment is repeated 100 times for statistical relevance. For each iteration:

\begin{itemize}
 \item [1.] The dataset is sampled to 20\% of its original size using \texttt{BasicLeverageScoresSampler}.
 \item [2.] The sampled dataset is split into training and test sets.
 \item [3.] Features are scaled using \texttt{MinMaxScaler}.
 \item [4.] The \texttt{SlowLogisticRegression} model is initialized and trained on the scaled training data, with training time recorded.
 \item [5.] Predictions are generated on the scaled test set.
 \item [6.] Accuracy is calculated by comparing predictions to the true test labels.
 \item [7.] Accuracy and training time are tracked separately.
\end{itemize}

This procedure records the accuracy scores and training times across each cycle, which are later used to calculate the average values for each metric.

\subsubsection{Analysis}

The collected accuracies and training times enable statistical analysis of the model's performance. Key statistics, such as the mean and standard deviation of accuracy, can be computed to evaluate consistency and generalization. Similarly, training time statistics provide insight into the computational cost of the \texttt{SlowLogisticRegression} implementation with leverage score sampling.

\subsubsection{Assumptions and Limitations}

The experiment assumes that the dataset is suitable for binary classification and that the features benefit from min-max scaling. The fixed hyperparameters (learning rate = 0.1, epochs = 5000), obtained empirically, appear to suit this experiment well and provide satisfactory results. Additionally, the use of leverage score sampling assumes that the selected 20\% subset adequately represents the original dataset's structure.

\subsubsection{Expected Outcomes}

The experiment is expected to yield a distribution of accuracy scores reflecting the model's predictive capability and a distribution of training times indicating its efficiency. These results can be used to benchmark \texttt{SlowLogisticRegression} against other implementations or algorithms under identical conditions, and particularly to evaluate the impact of leverage score sampling algorithm \texttt{BasicLeverageScoresSampler} impacts on prediction accuracy and model training time.

\subsubsection{Measured Outcomes}

The expected outcomes for this experiment have been empirically confirmed, as illustrated in \textbf{Figure 2}.

\begin{figure}[ht]
	\vskip 0.2in
	\begin{center}
		\centerline{\includegraphics[width=\columnwidth]{experiment_02}}
		\caption{Mean accuracies and training times observed over 100 training cycles of \texttt{SlowLogisticRegression}, with \texttt{BasicLeverageScoresSampler} sampling, using the Breast Cancer Wisconsin Diagnostic Dataset \cite{breastcancer}.}
		\label{experiment_02}
	\end{center}
	\vskip -0.2in
\end{figure}


\subsection{Experiments 3 and 4: Repeating Experiments 1 and 2 using SciKit-Learn's \texttt{LogisticRegression} Model}

\subsubsection{Objective}

Experiments 3 and 4 replicate Experiments 1 and 2, respectively, replacing \texttt{SlowLogisticRegression} with SciKit-Learn's optimized \texttt{LogisticRegression} class. Experiment 3 uses the full dataset, while Experiment 4 applies 20\% leverage score sampling via \texttt{BasicLeverageScoresSampler}. The goal is to assess whether sampling impacts the performance of an optimized model, comparing accuracy and training efficiency against Experiments 1 and 2 to evaluate sampling effects across custom and optimized implementations.

\subsubsection{Materials and Methods}

The dataset, sampling (for Experiment 4), and scaling follow the procedures in Experiments 1 and 2, using $ X \in \mathbb{R}^{n \times m} $, $ y \in \{0, 1\}^n $, an 80-20 train-test split (random seed 42), and \texttt{MinMaxScaler} normalization.

\subsubsection{Model and Training}

The model is SciKit-Learn's \texttt{LogisticRegression} with default parameters, replacing the custom \texttt{SlowLogisticRegression} (learning rate 0.1, 5000 epochs) used previously.

\subsubsection{Experimental Procedure}

The experiment is repeated 100 times for statistical relevance. The same procedure detailed for previous experiments are employed here for Experiment 3 and 4.

\subsubsection{Analysis}

Mean and standard deviation of accuracy and training times assess consistency, generalization, and computational cost, comparing optimized \texttt{LogisticRegression} performance with and without sampling.

\subsubsection{Assumptions and Limitations}

The experiment assumes binary classification suitability and effective min-max scaling. Default \texttt{LogisticRegression} parameters are presumed optimal, and sampling assumes the 20\% subset retains dataset structure.

\subsubsection{Expected Outcomes}

Accuracy and training time distributions will reveal \texttt{LogisticRegression}'s performance, highlighting sampling impacts versus Experiments 1 and 2, and benchmarking optimized versus custom models.

\subsubsection{Measured Outcomes}

The expected outcomes for this experiment have been empirically confirmed, as illustrated in \textbf{Figure 3} and \textbf{Figure 4}.

\begin{figure}[ht]
	\vskip 0.2in
	\begin{center}
		\centerline{\includegraphics[width=\columnwidth]{experiment_03}}
		\caption{Mean accuracies and training times observed over 100 training cycles of SciKit-Learn's \texttt{LogisticRegression} class, without any sampling, using the Breast Cancer Wisconsin Diagnostic Dataset \cite{breastcancer}.}
		\label{experiment_03}
	\end{center}
	\vskip -0.2in
\end{figure}

\begin{figure}[ht]
	\vskip 0.2in
	\begin{center}
		\centerline{\includegraphics[width=\columnwidth]{experiment_04}}
		\caption{Mean accuracies and training times observed over 100 training cycles of SciKit-Learn's \texttt{LogisticRegression} class, with \texttt{BasicLeverageScoresSampler} sampling, using the Breast Cancer Wisconsin Diagnostic Dataset \cite{breastcancer}.}
		\label{experiment_04}
	\end{center}
	\vskip -0.2in
\end{figure}


\subsection{Experiment 5: \texttt{SlowLogisticRegression}'s Performance with Random Sampling}

\subsubsection{Objective}

The experiment aims to assess the performance of a custom implementation of logistic regression, named \texttt{SlowLogisticRegression}, in terms of classification accuracy and training time across multiple runs on an open-source dataset \cite{breastcancer}. The experiment seeks to quantify the model's generalization ability and computational efficiency under standardized conditions, incorporating a data sampling step using the randomized sampling algorithm \texttt{RandomSampler}.


\subsubsection{Materials and Methods}

The experiment utilizes a dataset represented by feature matrix $ X \in \mathbb{R}^{n \times m} $ and target vector $ y \in \{0, 1\}^n $, where $ n $ is the number of samples and $ m $ is the number of features. The dataset is assumed to be preprocessed and suitable for binary classification tasks.

For each experimental run, the dataset is first sampled to 20\% of its original size using the \texttt{RandomSampler}, resulting in a reduced dataset ($ X_{\text{sampled}}, y_{\text{sampled}} $). This sampled dataset is then split into training and test sets using an 80-20 partition, with 80\% of the sampled data allocated to training ($ X_{\text{train}}, y_{\text{train}} $) and 20\% to testing ($ X_{\text{test}}, y_{\text{test}} $). The split is performed using a fixed random seed of 42 to ensure reproducibility across runs. Feature scaling is applied using Python's SciKit-Learn \texttt{MinMaxScaler}, which normalizes each feature to the range $[0, 1]$ based on the training set's minimum and maximum values. The scaling transformation is computed as:
\[
X'_{i,j} = \frac{X_{i,j} - \min(X_{:,j})}{\max(X_{:,j}) - \min(X_{:,j})}
\]
where $ X_{i,j} $ denotes the $ j $-th feature of the $ i $-th sample, and the transformation is fitted on $ X_{\text{train}} $ and applied to both $ X_{\text{train}} $ and $ X_{\text{test}} $.

\subsubsection{Model and Training}

The classification model employed is a custom logistic regression implementation (\texttt{SlowLogisticRegression}), parameterized by a learning rate of 0.1 and a fixed number of 5000 training epochs. The implementation of this model is detailed in section 2 of this paper.

\subsubsection{Experimental Procedure}

The experiment is repeated 100 times for statistical relevance. For each iteration:

\begin{itemize}
	\item [1.] The dataset is sampled to 20\% of its original size using \texttt{RandomSampler}.
	\item [2.] The sampled dataset is split into training and test sets.
	\item [3.] Features are scaled using \texttt{MinMaxScaler}.
	\item [4.] The \texttt{SlowLogisticRegression} model is initialized and trained on the scaled training data, with training time recorded.
	\item [5.] Predictions are generated on the scaled test set.
	\item [6.] Accuracy is calculated by comparing predictions to the true test labels.
	\item [7.] Accuracy and training time are tracked separately.
\end{itemize}

This procedure records the accuracy scores and training times across each cycle, which are later used to calculate the average values for each metric.

\subsubsection{Analysis}

The collected accuracies and training times enable statistical analysis of the model's performance. Key statistics, such as the mean and standard deviation of accuracy, can be computed to evaluate consistency and generalization. Similarly, training time statistics provide insight into the computational cost of the \texttt{SlowLogisticRegression} implementation with leverage score sampling.

\subsubsection{Assumptions and Limitations}

The experiment assumes that the dataset is suitable for binary classification and that the features benefit from min-max scaling. The fixed hyperparameters (learning rate = 0.1, epochs = 5000), obtained empirically, appear to suit this experiment well and provide satisfactory results. Additionally, the use of random sampling assumes that the selected 20\% subset adequately represents the original dataset's structure.

\subsubsection{Expected Outcomes}

The experiment is expected to yield a distribution of accuracy scores reflecting the model's predictive capability and a distribution of training times indicating its efficiency. These results can be used to benchmark \texttt{SlowLogisticRegression} against other implementations or algorithms under identical conditions, and particularly to evaluate the impact of random sampling algorithm \texttt{RandomSampler} on prediction accuracy and model training time.


\subsubsection{Measured Outcomes}

The expected outcomes for this experiment have been empirically confirmed, as illustrated in \textbf{Figure 5}.

\begin{figure}[ht]
	\vskip 0.2in
	\begin{center}
		\centerline{\includegraphics[width=\columnwidth]{experiment_05}}
		\caption{Mean accuracies and training times observed over 100 training cycles of \texttt{SlowLogisticRegression}, with \texttt{RandomSampler} sampling, using the Breast Cancer Wisconsin Diagnostic Dataset \cite{breastcancer}.}
		\label{experiment_05}
	\end{center}
	\vskip -0.2in
\end{figure}


\subsection{Experiment 6: Statistical Validation of Experiments 1–5 with Increased Runs}

\subsubsection{Objective}

Experiment 5 re-executes Experiments 1, 2, 3, 4, and 5, increasing the number of cycles from 100 to 1000 per condition, to enhance the statistical relevance of prior observations. It assesses \texttt{SlowLogisticRegression} and \texttt{LogisticRegression} performance under full and 20\% sampled dataset scenarios, aiming to confirm the consistency and reliability of accuracy and training time findings from earlier experiments.

\subsubsection{Materials and Methods}

The dataset, sampling (for Experiments 2 and 4), and scaling remain as in Experiments 1–5, using $ X \in \mathbb{R}^{n \times m} $, $ y \in \{0, 1\}^n $, an 80-20 train-test split (random seed 42), and \texttt{MinMaxScaler} normalization.

\subsubsection{Model and Training}

Models are \texttt{SlowLogisticRegression} (learning rate 0.1, 5000 epochs) for Experiments 1, 2, and 5, and SciKit-Learn's \texttt{LogisticRegression} (default parameters) for Experiments 3 and 4, as previously defined.

\subsubsection{Experimental Procedure}

The experiment is repeated 1000 times per condition (Experiments 1–5). The same procedures described earlier for each experiment still apply here.

\subsubsection{Analysis}

Mean and standard deviation of accuracy and training times, based on 1000 runs, provide robust statistical validation of consistency, generalization, and computational cost across all conditions.

\subsubsection{Assumptions and Limitations}

Assumptions align with Experiments 1–5: binary classification suitability, effective scaling, and representative sampling. Fixed parameters for \texttt{SlowLogisticRegression} and defaults for \texttt{LogisticRegression} are presumed suitable.

\subsubsection{Expected Outcomes}

Distributions of accuracy and training times across 1000 runs will confirm the statistical relevance of Experiments 1–5, reinforcing comparisons between models and sampling effects with greater confidence.

\subsubsection{Measured Outcomes}

The expected outcomes for this experiment have been empirically confirmed, as illustrated in \textbf{Figure 6}.

\begin{figure}[ht]
	\vskip 0.2in
	\begin{center}
		\centerline{\includegraphics[width=\columnwidth]{experiment_06}}
		\caption{Mean accuracies and training times observed when repeating experiments 1, 2, 3, 4, and 5, this time with 1000 training cycles each.}
		\label{experiment_06}
	\end{center}
	\vskip -0.2in
\end{figure}


\subsection{Experiment 7: SciKit-Learn's \texttt{KNeighborsClassifier} Model's Performance, No Sampling}

\subsubsection{Objective}

The experiment aims to assess the performance of SciKit-Learn's K-Nearest Neighbors model, named \texttt{KNeighborsClassifier}, in terms of classification accuracy and training time across multiple runs on an open-source dataset \cite{iris}. The experiment seeks to quantify the model's generalization ability and computational efficiency under standardized conditions.


\subsubsection{Materials and Methods}

The experiment utilizes a dataset represented by feature matrix $ X \in \mathbb{R}^{n \times m} $ and target vector $ y \in \{0, 1\}^n $, where $ n $ is the number of samples and $ m $ is the number of features. The dataset is assumed to be preprocessed and suitable for classification tasks.

For each experimental run, the dataset is split into training and test sets using an 80-20 partition, with 80\% of the data allocated to training ($ X_{\text{train}}, y_{\text{train}} $) and 20\% to testing ($ X_{\text{test}}, y_{\text{test}} $). The split is performed using a fixed random seed of 42 to ensure reproducibility across runs. Feature scaling is not used in this experiment.


\subsubsection{Model and Training}

The classification model employed is provided by SciKit-Learn's \texttt{KNeighborsClassifier} class.


\subsubsection{Experimental Procedure}

The experiment is repeated 1000 times for statistical relevance. For each iteration:

\begin{itemize}
	\item [1.] The dataset is split into training and test sets.
	\item [2.] The \texttt{KNeighborsClassifier} model is initialized and trained, with training time recorded.
	\item [3.] Predictions are generated on the test set.
	\item [4.] Accuracy is calculated by comparing predictions to the true test labels.
	\item [5.] Accuracy and training time are tracked separately.
\end{itemize}

This procedure records the accuracy scores and training times across each cycle, which are later used to calculate the average values for each metric.


\subsubsection{Analysis}

The collected accuracies and training times enable statistical analysis of the model's performance. Key statistics, such as the mean and standard deviation of accuracy, can be computed to evaluate consistency and generalization. Similarly, training time statistics provide insight into the computational cost of the \texttt{KNeighborsClassifier} implementation without any sampling.


\subsubsection{Assumptions and Limitations}

The experiment assumes that the dataset is suitable for classification and that the features.


\subsubsection{Expected Outcomes}

The experiment is expected to yield a distribution of accuracy scores reflecting the model's predictive capability and a distribution of training times indicating its efficiency. These results can be used to benchmark \texttt{KNeighborsClassifier} against other implementations or algorithms under identical conditions.

\subsubsection{Measured Outcomes}

The expected outcomes for this experiment have been empirically confirmed, as illustrated in \textbf{Figure 7}.

\begin{figure}[ht]
	\vskip 0.2in
	\begin{center}
		\centerline{\includegraphics[width=\columnwidth]{experiment_07}}
		\caption{Mean accuracies and training times observed over 1000 training cycles of \texttt{KNeighborsClassifier}, without any sampling, using the Iris Dataset \cite{iris}.}
		\label{experiment_07}
	\end{center}
	\vskip -0.2in
\end{figure}


\subsection{Experiment 8: SciKit-Learn's \texttt{KNeighborsClassifier} Model's Performance with Leverage Score Sampling}

\subsubsection{Objective}

The experiment aims to assess the performance of SciKit-Learn's \texttt{KNeighborsClassifier} model, in terms of classification accuracy and training time across multiple runs on an open-source dataset \cite{iris}. The experiment seeks to quantify the model's generalization ability and computational efficiency under standardized conditions, incorporating a data sampling step using the randomized sampling algorithm \texttt{BasicLeverageScoresSampler}.


\subsubsection{Materials and Methods}

The experiment utilizes a dataset represented by feature matrix $ X \in \mathbb{R}^{n \times m} $ and target vector $ y \in \{0, 1\}^n $, where $ n $ is the number of samples and $ m $ is the number of features. The dataset is assumed to be preprocessed and suitable for classification tasks.

For each experimental run, the dataset is first sampled to 20\% of its original size using the \texttt{BasicLeverageScoresSampler}, resulting in a reduced dataset ($ X_{\text{sampled}}, y_{\text{sampled}} $). This sampled dataset is then split into training and test sets using an 80-20 partition, with 80\% of the sampled data allocated to training ($ X_{\text{train}}, y_{\text{train}} $) and 20\% to testing ($ X_{\text{test}}, y_{\text{test}} $). The split is performed using a fixed random seed of 42 to ensure reproducibility across runs. Feature scaling is not used in this experiment.

\subsubsection{Model and Training}

The classification model employed is provided by SciKit-Learn's \texttt{KNeighborsClassifier} class.

\subsubsection{Experimental Procedure}

The experiment is repeated 1000 times for statistical relevance. For each iteration:

\begin{itemize}
	\item [1.] The dataset is sampled to 20\% of its original size using \texttt{BasicLeverageScoresSampler}.
	\item [2.] The sampled dataset is split into training and test sets.
	\item [3.] The \texttt{KNeighborsClassifier} model is initialized and trained on the training data, with training time recorded.
	\item [4.] Predictions are generated on the test set.
	\item [5.] Accuracy is calculated by comparing predictions to the true test labels.
	\item [6.] Accuracy and training time are tracked separately.
\end{itemize}

This procedure records the accuracy scores and training times across each cycle, which are later used to calculate the average values for each metric.

\subsubsection{Analysis}

The collected accuracies and training times enable statistical analysis of the model's performance. Key statistics, such as the mean and standard deviation of accuracy, can be computed to evaluate consistency and generalization. Similarly, training time statistics provide insight into the computational cost of the \texttt{KNeighborsClassifier} implementation with leverage score sampling.

\subsubsection{Assumptions and Limitations}

The experiment assumes that the dataset is suitable for classification. Additionally, the use of leverage score sampling assumes that the selected 20\% subset adequately represents the original dataset's structure.

\subsubsection{Expected Outcomes}

The experiment is expected to yield a distribution of accuracy scores reflecting the model's predictive capability and a distribution of training times indicating its efficiency. These results can be used to benchmark \texttt{KNeighborsClassifier} against other implementations or algorithms under identical conditions, and particularly to evaluate the impact of leverage score sampling algorithm \texttt{BasicLeverageScoresSampler} impacts on prediction accuracy and model training time.

\subsubsection{Measured Outcomes}

The expected outcomes for this experiment have been empirically confirmed, as illustrated in \textbf{Figure 8}.

\begin{figure}[ht]
	\vskip 0.2in
	\begin{center}
		\centerline{\includegraphics[width=\columnwidth]{experiment_08}}
		\caption{Mean accuracies and training times observed over 1000 training cycles of \texttt{KNeighborsClassifier}, with \texttt{BasicLeverageScoresSampler} sampling, using the Iris Dataset \cite{iris}.}
		\label{experiment_08}
	\end{center}
	\vskip -0.2in
\end{figure}


\subsection{Experiment 9: SciKit-Learn's \texttt{KNeighborsClassifier} Model's Performance with Random Sampling}

\subsubsection{Objective}

The experiment aims to assess the performance of SciKit-Learn's \texttt{KNeighborsClassifier} model, in terms of classification accuracy and training time across multiple runs on an open-source dataset \cite{iris}. The experiment seeks to quantify the model's generalization ability and computational efficiency under standardized conditions, incorporating a data sampling step using the randomized sampling algorithm \texttt{RandomSampler}.


\subsubsection{Materials and Methods}

The experiment utilizes a dataset represented by feature matrix $ X \in \mathbb{R}^{n \times m} $ and target vector $ y \in \{0, 1\}^n $, where $ n $ is the number of samples and $ m $ is the number of features. The dataset is assumed to be preprocessed and suitable for classification tasks.

For each experimental run, the dataset is first sampled to 20\% of its original size using the \texttt{RandomSampler}, resulting in a reduced dataset ($ X_{\text{sampled}}, y_{\text{sampled}} $). This sampled dataset is then split into training and test sets using an 80-20 partition, with 80\% of the sampled data allocated to training ($ X_{\text{train}}, y_{\text{train}} $) and 20\% to testing ($ X_{\text{test}}, y_{\text{test}} $). The split is performed using a fixed random seed of 42 to ensure reproducibility across runs. Feature scaling is not used in this experiment.

\subsubsection{Model and Training}

The classification model employed is provided by SciKit-Learn's \texttt{KNeighborsClassifier} class.

\subsubsection{Experimental Procedure}

The experiment is repeated 1000 times for statistical relevance. For each iteration:

\begin{itemize}
	\item [1.] The dataset is sampled to 20\% of its original size using \texttt{RandomSampler}.
	\item [2.] The sampled dataset is split into training and test sets.
	\item [3.] The \texttt{KNeighborsClassifier} model is initialized and trained on the training data, with training time recorded.
	\item [4.] Predictions are generated on the test set.
	\item [5.] Accuracy is calculated by comparing predictions to the true test labels.
	\item [6.] Accuracy and training time are tracked separately.
\end{itemize}

This procedure records the accuracy scores and training times across each cycle, which are later used to calculate the average values for each metric.

\subsubsection{Analysis}

The collected accuracies and training times enable statistical analysis of the model's performance. Key statistics, such as the mean and standard deviation of accuracy, can be computed to evaluate consistency and generalization. Similarly, training time statistics provide insight into the computational cost of the \texttt{KNeighborsClassifier} implementation with random sampling.

\subsubsection{Assumptions and Limitations}

The experiment assumes that the dataset is suitable for classification. Additionally, the use of random sampling assumes that the selected 20\% subset adequately represents the original dataset's structure.

\subsubsection{Expected Outcomes}

The experiment is expected to yield a distribution of accuracy scores reflecting the model's predictive capability and a distribution of training times indicating its efficiency. These results can be used to benchmark \texttt{KNeighborsClassifier} against other implementations or algorithms under identical conditions, and particularly to evaluate the impact of leverage score sampling algorithm \texttt{RandomSampler} impacts on prediction accuracy and model training time.

\subsubsection{Measured Outcomes}

The expected outcomes for this experiment have been empirically confirmed, as illustrated in \textbf{Figure 9}.

\begin{figure}[ht]
	\vskip 0.2in
	\begin{center}
		\centerline{\includegraphics[width=\columnwidth]{experiment_09}}
		\caption{Mean accuracies and training times observed over 1000 training cycles of \texttt{KNeighborsClassifier}, with \texttt{RandomSampler} sampling, using the Iris Dataset \cite{iris}.}
		\label{experiment_09}
	\end{center}
	\vskip -0.2in
\end{figure}


\subsection{Experiment 10: SciKit-Learn's \texttt{SVC} Model's Performance, No Sampling}

\subsubsection{Objective}

The experiment aims to assess the performance of SciKit-Learn's Support Vector Machine model, named \texttt{SVC}, in terms of classification accuracy and training time across multiple runs on an open-source dataset \cite{iris}. The experiment seeks to quantify the model's generalization ability and computational efficiency under standardized conditions.


\subsubsection{Materials and Methods}

The experiment utilizes a dataset represented by feature matrix $ X \in \mathbb{R}^{n \times m} $ and target vector $ y \in \{0, 1\}^n $, where $ n $ is the number of samples and $ m $ is the number of features. The dataset is assumed to be preprocessed and suitable for classification tasks.

For each experimental run, the dataset is split into training and test sets using an 80-20 partition, with 80\% of the data allocated to training ($ X_{\text{train}}, y_{\text{train}} $) and 20\% to testing ($ X_{\text{test}}, y_{\text{test}} $). The split is performed using a fixed random seed of 42 to ensure reproducibility across runs. Feature scaling is not used in this experiment.


\subsubsection{Model and Training}

The classification model employed is provided by SciKit-Learn's \texttt{SVC} class.


\subsubsection{Experimental Procedure}

The experiment is repeated 1000 times for statistical relevance. For each iteration:

\begin{itemize}
	\item [1.] The dataset is split into training and test sets.
	\item [2.] The \texttt{SVC} model is initialized and trained, with training time recorded.
	\item [3.] Predictions are generated on the test set.
	\item [4.] Accuracy is calculated by comparing predictions to the true test labels.
	\item [5.] Accuracy and training time are tracked separately.
\end{itemize}

This procedure records the accuracy scores and training times across each cycle, which are later used to calculate the average values for each metric.


\subsubsection{Analysis}

The collected accuracies and training times enable statistical analysis of the model's performance. Key statistics, such as the mean and standard deviation of accuracy, can be computed to evaluate consistency and generalization. Similarly, training time statistics provide insight into the computational cost of the \texttt{SVC} implementation without any sampling.


\subsubsection{Assumptions and Limitations}

The experiment assumes that the dataset is suitable for classification and that the features.


\subsubsection{Expected Outcomes}

The experiment is expected to yield a distribution of accuracy scores reflecting the model's predictive capability and a distribution of training times indicating its efficiency. These results can be used to benchmark \texttt{SVC} against other implementations or algorithms under identical conditions.

\subsubsection{Measured Outcomes}

The expected outcomes for this experiment have been empirically confirmed, as illustrated in \textbf{Figure 10}.

\begin{figure}[ht]
	\vskip 0.2in
	\begin{center}
		\centerline{\includegraphics[width=\columnwidth]{experiment_10}}
		\caption{Mean accuracies and training times observed over 1000 training cycles of \texttt{SVC}, without any sampling, using the Iris Dataset \cite{iris}.}
		\label{experiment_10}
	\end{center}
	\vskip -0.2in
\end{figure}


\subsection{Experiment 11: SciKit-Learn's \texttt{SVC} Model's Performance with Leverage Score Sampling}

\subsubsection{Objective}

The experiment aims to assess the performance of SciKit-Learn's \texttt{SVC} model, in terms of classification accuracy and training time across multiple runs on an open-source dataset \cite{iris}. The experiment seeks to quantify the model's generalization ability and computational efficiency under standardized conditions, incorporating a data sampling step using the randomized sampling algorithm \texttt{BasicLeverageScoresSampler}.


\subsubsection{Materials and Methods}

The experiment utilizes a dataset represented by feature matrix $ X \in \mathbb{R}^{n \times m} $ and target vector $ y \in \{0, 1\}^n $, where $ n $ is the number of samples and $ m $ is the number of features. The dataset is assumed to be preprocessed and suitable for classification tasks.

For each experimental run, the dataset is first sampled to 20\% of its original size using the \texttt{BasicLeverageScoresSampler}, resulting in a reduced dataset ($ X_{\text{sampled}}, y_{\text{sampled}} $). This sampled dataset is then split into training and test sets using an 80-20 partition, with 80\% of the sampled data allocated to training ($ X_{\text{train}}, y_{\text{train}} $) and 20\% to testing ($ X_{\text{test}}, y_{\text{test}} $). The split is performed using a fixed random seed of 42 to ensure reproducibility across runs. Feature scaling is not used in this experiment.

\subsubsection{Model and Training}

The classification model employed is provided by SciKit-Learn's \texttt{SVC} class.

\subsubsection{Experimental Procedure}

The experiment is repeated 1000 times for statistical relevance. For each iteration:

\begin{itemize}
	\item [1.] The dataset is sampled to 20\% of its original size using \texttt{BasicLeverageScoresSampler}.
	\item [2.] The sampled dataset is split into training and test sets.
	\item [3.] The \texttt{SVC} model is initialized and trained on the training data, with training time recorded.
	\item [4.] Predictions are generated on the test set.
	\item [5.] Accuracy is calculated by comparing predictions to the true test labels.
	\item [6.] Accuracy and training time are tracked separately.
\end{itemize}

This procedure records the accuracy scores and training times across each cycle, which are later used to calculate the average values for each metric.

\subsubsection{Analysis}

The collected accuracies and training times enable statistical analysis of the model's performance. Key statistics, such as the mean and standard deviation of accuracy, can be computed to evaluate consistency and generalization. Similarly, training time statistics provide insight into the computational cost of the \texttt{SVC} implementation with leverage score sampling.

\subsubsection{Assumptions and Limitations}

The experiment assumes that the dataset is suitable for classification. Additionally, the use of leverage score sampling assumes that the selected 20\% subset adequately represents the original dataset's structure.

\subsubsection{Expected Outcomes}

The experiment is expected to yield a distribution of accuracy scores reflecting the model's predictive capability and a distribution of training times indicating its efficiency. These results can be used to benchmark \texttt{SVC} against other implementations or algorithms under identical conditions, and particularly to evaluate the impact of leverage score sampling algorithm \texttt{BasicLeverageScoresSampler} impacts on prediction accuracy and model training time.

\subsubsection{Measured Outcomes}

The expected outcomes for this experiment have been empirically confirmed, as illustrated in \textbf{Figure 11}.

\begin{figure}[ht]
	\vskip 0.2in
	\begin{center}
		\centerline{\includegraphics[width=\columnwidth]{experiment_11}}
		\caption{Mean accuracies and training times observed over 1000 training cycles of \texttt{SVC}, with \texttt{BasicLeverageScoresSampler} sampling, using the Iris Dataset \cite{iris}.}
		\label{experiment_11}
	\end{center}
	\vskip -0.2in
\end{figure}


\subsection{Experiment 12: SciKit-Learn's \texttt{SVC} Model's Performance with Random Sampling}

\subsubsection{Objective}

The experiment aims to assess the performance of SciKit-Learn's \texttt{SVC} model, in terms of classification accuracy and training time across multiple runs on an open-source dataset \cite{iris}. The experiment seeks to quantify the model's generalization ability and computational efficiency under standardized conditions, incorporating a data sampling step using the randomized sampling algorithm \texttt{RandomSampler}.


\subsubsection{Materials and Methods}

The experiment utilizes a dataset represented by feature matrix $ X \in \mathbb{R}^{n \times m} $ and target vector $ y \in \{0, 1\}^n $, where $ n $ is the number of samples and $ m $ is the number of features. The dataset is assumed to be preprocessed and suitable for classification tasks.

For each experimental run, the dataset is first sampled to 20\% of its original size using the \texttt{RandomSampler}, resulting in a reduced dataset ($ X_{\text{sampled}}, y_{\text{sampled}} $). This sampled dataset is then split into training and test sets using an 80-20 partition, with 80\% of the sampled data allocated to training ($ X_{\text{train}}, y_{\text{train}} $) and 20\% to testing ($ X_{\text{test}}, y_{\text{test}} $). The split is performed using a fixed random seed of 42 to ensure reproducibility across runs. Feature scaling is not used in this experiment.

\subsubsection{Model and Training}

The classification model employed is provided by SciKit-Learn's \texttt{SVC} class.

\subsubsection{Experimental Procedure}

The experiment is repeated 1000 times for statistical relevance. For each iteration:

\begin{itemize}
	\item [1.] The dataset is sampled to 20\% of its original size using \texttt{RandomSampler}.
	\item [2.] The sampled dataset is split into training and test sets.
	\item [3.] The \texttt{SVC} model is initialized and trained on the training data, with training time recorded.
	\item [4.] Predictions are generated on the test set.
	\item [5.] Accuracy is calculated by comparing predictions to the true test labels.
	\item [6.] Accuracy and training time are tracked separately.
\end{itemize}

This procedure records the accuracy scores and training times across each cycle, which are later used to calculate the average values for each metric.

\subsubsection{Analysis}

The collected accuracies and training times enable statistical analysis of the model's performance. Key statistics, such as the mean and standard deviation of accuracy, can be computed to evaluate consistency and generalization. Similarly, training time statistics provide insight into the computational cost of the \texttt{SVC} implementation with random sampling.

\subsubsection{Assumptions and Limitations}

The experiment assumes that the dataset is suitable for classification. Additionally, the use of random sampling assumes that the selected 20\% subset adequately represents the original dataset's structure.

\subsubsection{Expected Outcomes}

The experiment is expected to yield a distribution of accuracy scores reflecting the model's predictive capability and a distribution of training times indicating its efficiency. These results can be used to benchmark \texttt{SVC} against other implementations or algorithms under identical conditions, and particularly to evaluate the impact of leverage score sampling algorithm \texttt{RandomSampler} impacts on prediction accuracy and model training time.

\subsubsection{Measured Outcomes}

The expected outcomes for this experiment have been empirically confirmed, as illustrated in \textbf{Figure 12}.

\begin{figure}[ht]
	\vskip 0.2in
	\begin{center}
		\centerline{\includegraphics[width=\columnwidth]{experiment_12}}
		\caption{Mean accuracies and training times observed over 1000 training cycles of \texttt{SVC}, with \texttt{RandomSampler} sampling, using the Iris Dataset \cite{iris}.}
		\label{experiment_12}
	\end{center}
	\vskip -0.2in
\end{figure}


\subsection{Experiment 13: SciKit-Learn's \texttt{RandomForestClassifier} Model's Performance, No Sampling}

\subsubsection{Objective}

The experiment aims to assess the performance of SciKit-Learn's Random Forest Classifier model, named \texttt{RandomForestClassifier}, in terms of classification accuracy and training time across multiple runs on an open-source dataset \cite{wine}. The experiment seeks to quantify the model's generalization ability and computational efficiency under standardized conditions.


\subsubsection{Materials and Methods}

The experiment utilizes a dataset represented by feature matrix $ X \in \mathbb{R}^{n \times m} $ and target vector $ y \in \{0, 1\}^n $, where $ n $ is the number of samples and $ m $ is the number of features. The dataset is assumed to be preprocessed and suitable for classification tasks.

For each experimental run, the dataset is split into training and test sets using an 80-20 partition, with 80\% of the data allocated to training ($ X_{\text{train}}, y_{\text{train}} $) and 20\% to testing ($ X_{\text{test}}, y_{\text{test}} $). The split is performed using a fixed random seed of 42 to ensure reproducibility across runs. Feature scaling is not used in this experiment.


\subsubsection{Model and Training}

The classification model employed is provided by SciKit-Learn's \texttt{RandomForestClassifier} class.


\subsubsection{Experimental Procedure}

The experiment is repeated 1000 times for statistical relevance. For each iteration:

\begin{itemize}
	\item [1.] The dataset is split into training and test sets.
	\item [2.] The \texttt{RandomForestClassifier} model is initialized and trained, with training time recorded.
	\item [3.] Predictions are generated on the test set.
	\item [4.] Accuracy is calculated by comparing predictions to the true test labels.
	\item [5.] Accuracy and training time are tracked separately.
\end{itemize}

This procedure records the accuracy scores and training times across each cycle, which are later used to calculate the average values for each metric.


\subsubsection{Analysis}

The collected accuracies and training times enable statistical analysis of the model's performance. Key statistics, such as the mean and standard deviation of accuracy, can be computed to evaluate consistency and generalization. Similarly, training time statistics provide insight into the computational cost of the \texttt{RandomForestClassifier} implementation without any sampling.


\subsubsection{Assumptions and Limitations}

The experiment assumes that the dataset is suitable for classification and that the features.


\subsubsection{Expected Outcomes}

The experiment is expected to yield a distribution of accuracy scores reflecting the model's predictive capability and a distribution of training times indicating its efficiency. These results can be used to benchmark \texttt{RandomForestClassifier} against other implementations or algorithms under identical conditions.

\subsubsection{Measured Outcomes}

The expected outcomes for this experiment have been empirically confirmed, as illustrated in \textbf{Figure 13}.

\begin{figure}[ht]
	\vskip 0.2in
	\begin{center}
		\centerline{\includegraphics[width=\columnwidth]{experiment_13}}
		\caption{Mean accuracies and training times observed over 1000 training cycles of \texttt{RandomForestClassifier}, without any sampling, using the Wine Dataset \cite{wine}.}
		\label{experiment_13}
	\end{center}
	\vskip -0.2in
\end{figure}


\subsection{Experiment 14: SciKit-Learn's \texttt{RandomForestClassifier} Model's Performance with Leverage Score Sampling}

\subsubsection{Objective}

The experiment aims to assess the performance of SciKit-Learn's \texttt{RandomForestClassifier} model, in terms of classification accuracy and training time across multiple runs on an open-source dataset \cite{wine}. The experiment seeks to quantify the model's generalization ability and computational efficiency under standardized conditions, incorporating a data sampling step using the randomized sampling algorithm \texttt{BasicLeverageScoresSampler}.


\subsubsection{Materials and Methods}

The experiment utilizes a dataset represented by feature matrix $ X \in \mathbb{R}^{n \times m} $ and target vector $ y \in \{0, 1\}^n $, where $ n $ is the number of samples and $ m $ is the number of features. The dataset is assumed to be preprocessed and suitable for classification tasks.

For each experimental run, the dataset is first sampled to 20\% of its original size using the \texttt{BasicLeverageScoresSampler}, resulting in a reduced dataset ($ X_{\text{sampled}}, y_{\text{sampled}} $). This sampled dataset is then split into training and test sets using an 80-20 partition, with 80\% of the sampled data allocated to training ($ X_{\text{train}}, y_{\text{train}} $) and 20\% to testing ($ X_{\text{test}}, y_{\text{test}} $). The split is performed using a fixed random seed of 42 to ensure reproducibility across runs. Feature scaling is not used in this experiment.

\subsubsection{Model and Training}

The classification model employed is provided by SciKit-Learn's \texttt{RandomForestClassifier} class.

\subsubsection{Experimental Procedure}

The experiment is repeated 1000 times for statistical relevance. For each iteration:

\begin{itemize}
	\item [1.] The dataset is sampled to 20\% of its original size using \texttt{BasicLeverageScoresSampler}.
	\item [2.] The sampled dataset is split into training and test sets.
	\item [3.] The \texttt{RandomForestClassifier} model is initialized and trained on the training data, with training time recorded.
	\item [4.] Predictions are generated on the test set.
	\item [5.] Accuracy is calculated by comparing predictions to the true test labels.
	\item [6.] Accuracy and training time are tracked separately.
\end{itemize}

This procedure records the accuracy scores and training times across each cycle, which are later used to calculate the average values for each metric.

\subsubsection{Analysis}

The collected accuracies and training times enable statistical analysis of the model's performance. Key statistics, such as the mean and standard deviation of accuracy, can be computed to evaluate consistency and generalization. Similarly, training time statistics provide insight into the computational cost of the \texttt{RandomForestClassifier} implementation with leverage score sampling.

\subsubsection{Assumptions and Limitations}

The experiment assumes that the dataset is suitable for classification. Additionally, the use of leverage score sampling assumes that the selected 20\% subset adequately represents the original dataset's structure.

\subsubsection{Expected Outcomes}

The experiment is expected to yield a distribution of accuracy scores reflecting the model's predictive capability and a distribution of training times indicating its efficiency. These results can be used to benchmark \texttt{RandomForestClassifier} against other implementations or algorithms under identical conditions, and particularly to evaluate the impact of leverage score sampling algorithm \texttt{BasicLeverageScoresSampler} impacts on prediction accuracy and model training time.

\subsubsection{Measured Outcomes}

The expected outcomes for this experiment have been empirically confirmed, as illustrated in \textbf{Figure 14}.

\begin{figure}[ht]
	\vskip 0.2in
	\begin{center}
		\centerline{\includegraphics[width=\columnwidth]{experiment_14}}
		\caption{Mean accuracies and training times observed over 1000 training cycles of \texttt{RandomForestClassifier}, with \texttt{BasicLeverageScoresSampler} sampling, using the Wine Dataset \cite{wine}.}
		\label{experiment_14}
	\end{center}
	\vskip -0.2in
\end{figure}


\subsection{Experiment 15: SciKit-Learn's \texttt{RandomForestClassifier} Model's Performance with Random Sampling}

\subsubsection{Objective}

The experiment aims to assess the performance of SciKit-Learn's \texttt{RandomForestClassifier} model, in terms of classification accuracy and training time across multiple runs on an open-source dataset \cite{wine}. The experiment seeks to quantify the model's generalization ability and computational efficiency under standardized conditions, incorporating a data sampling step using the randomized sampling algorithm \texttt{RandomSampler}.


\subsubsection{Materials and Methods}

The experiment utilizes a dataset represented by feature matrix $ X \in \mathbb{R}^{n \times m} $ and target vector $ y \in \{0, 1\}^n $, where $ n $ is the number of samples and $ m $ is the number of features. The dataset is assumed to be preprocessed and suitable for classification tasks.

For each experimental run, the dataset is first sampled to 20\% of its original size using the \texttt{RandomSampler}, resulting in a reduced dataset ($ X_{\text{sampled}}, y_{\text{sampled}} $). This sampled dataset is then split into training and test sets using an 80-20 partition, with 80\% of the sampled data allocated to training ($ X_{\text{train}}, y_{\text{train}} $) and 20\% to testing ($ X_{\text{test}}, y_{\text{test}} $). The split is performed using a fixed random seed of 42 to ensure reproducibility across runs. Feature scaling is not used in this experiment.

\subsubsection{Model and Training}

The classification model employed is provided by SciKit-Learn's \texttt{RandomForestClassifier} class.

\subsubsection{Experimental Procedure}

The experiment is repeated 1000 times for statistical relevance. For each iteration:

\begin{itemize}
	\item [1.] The dataset is sampled to 20\% of its original size using \texttt{RandomSampler}.
	\item [2.] The sampled dataset is split into training and test sets.
	\item [3.] The \texttt{RandomForestClassifier} model is initialized and trained on the training data, with training time recorded.
	\item [4.] Predictions are generated on the test set.
	\item [5.] Accuracy is calculated by comparing predictions to the true test labels.
	\item [6.] Accuracy and training time are tracked separately.
\end{itemize}

This procedure records the accuracy scores and training times across each cycle, which are later used to calculate the average values for each metric.

\begin{figure}[ht]
	\vskip 0.2in
	\begin{center}
		\centerline{\includegraphics[width=\columnwidth]{experiment_15}}
		\caption{Mean accuracies and training times observed over 1000 training cycles of \texttt{RandomForestClassifier}, with \texttt{RandomSampler} sampling, using the Wine Dataset \cite{wine}.}
		\label{experiment_15}
	\end{center}
	\vskip -0.2in
\end{figure}

\subsubsection{Analysis}

The collected accuracies and training times enable statistical analysis of the model's performance. Key statistics, such as the mean and standard deviation of accuracy, can be computed to evaluate consistency and generalization. Similarly, training time statistics provide insight into the computational cost of the \texttt{SVC} implementation with random sampling.

\subsubsection{Assumptions and Limitations}

The experiment assumes that the dataset is suitable for classification. Additionally, the use of random sampling assumes that the selected 20\% subset adequately represents the original dataset's structure.

\subsubsection{Expected Outcomes}

The experiment is expected to yield a distribution of accuracy scores reflecting the model's predictive capability and a distribution of training times indicating its efficiency. These results can be used to benchmark \texttt{RandomForestClassifier} against other implementations or algorithms under identical conditions, and particularly to evaluate the impact of leverage score sampling algorithm \texttt{RandomSampler} impacts on prediction accuracy and model training time.

\subsubsection{Measured Outcomes}

The expected outcomes for this experiment have been empirically confirmed, as illustrated in \textbf{Figure 15}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}
